import io
import spaces
import torch
import librosa
import requests
import tempfile
import numpy as np
import gradio as gr
import soundfile as sf
from transformers import AutoModel

# Function to load reference audio from URL
def load_audio_from_url(url):
    response = requests.get(url)
    if response.status_code == 200:
        audio_data, sample_rate = sf.read(io.BytesIO(response.content))
        return sample_rate, audio_data
    return None, None

@spaces.GPU
def synthesize_speech(text, ref_audio, ref_text):
    if ref_audio is None or ref_text.strip() == "":
        return "Error: Please provide a reference audio and its corresponding text."
    
    # Ensure valid reference audio input
    if isinstance(ref_audio, tuple) and len(ref_audio) == 2:
        sample_rate, audio_data = ref_audio
    else:
        return "Error: Invalid reference audio input."
    
    # Save reference audio directly without resampling
    with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as temp_audio:
        sf.write(temp_audio.name, audio_data, samplerate=sample_rate, format='WAV')
        temp_audio.flush()
    
    audio = model(text, ref_audio_path=temp_audio.name, ref_text=ref_text)
             
    # Normalize output and save
    if audio.dtype == np.int16:
        audio = audio.astype(np.float32) / 32768.0

    return 24000, audio


# Load TTS model
repo_id = "ai4bharat/IndicF5"
model = AutoModel.from_pretrained(repo_id, trust_remote_code=True)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Device", device)
model = model.to(device)

# Example Data (Multiple Examples)
EXAMPLES = [
    {
        "audio_name": "PAN_F (Happy)",
        "audio_url": "https://github.com/AI4Bharat/IndicF5/raw/refs/heads/main/prompts/PAN_F_HAPPY_00001.wav",
        "ref_text": "‡®≠‡®π‡©∞‡®™‡©Ä ‡®µ‡®ø‡©±‡®ö ‡®∏‡®Æ‡®æ‡®∞‡®ï‡®æ‡®Ç ‡®¶‡©á ‡®≠‡®µ‡®® ‡®®‡®ø‡®∞‡®Æ‡®æ‡®£ ‡®ï‡®≤‡®æ ‡®¶‡©á ‡®µ‡©á‡®∞‡®µ‡©á ‡®ó‡©Å‡©∞‡®ù‡®≤‡®¶‡®æ‡®∞ ‡®Ö‡®§‡©á ‡®π‡©à‡®∞‡®æ‡®® ‡®ï‡®∞‡®® ‡®µ‡®æ‡®≤‡©á ‡®π‡®®, ‡®ú‡©ã ‡®Æ‡©à‡®®‡©Ç‡©∞ ‡®ñ‡©Å‡®∏‡®º ‡®ï‡®∞‡®¶‡©á  ‡®π‡®®‡•§",
        "synth_text": "‡§Æ‡•à‡§Ç ‡§¨‡§ø‡§®‡§æ ‡§ï‡§ø‡§∏‡•Ä ‡§ö‡§ø‡§Ç‡§§‡§æ ‡§ï‡•á ‡§Ö‡§™‡§®‡•á ‡§¶‡•ã‡§∏‡•ç‡§§‡•ã‡§Ç ‡§ï‡•ã ‡§Ö‡§™‡§®‡•á ‡§ë‡§ü‡•ã‡§Æ‡•ã‡§¨‡§æ‡§á‡§≤ ‡§è‡§ï‡•ç‡§∏‡§™‡§∞‡•ç‡§ü ‡§ï‡•á ‡§™‡§æ‡§∏ ‡§≠‡•á‡§ú ‡§¶‡•á‡§§‡§æ ‡§π‡•Ç‡§Å ‡§ï‡•ç‡§Ø‡•ã‡§Ç‡§ï‡§ø ‡§Æ‡•à‡§Ç ‡§ú‡§æ‡§®‡§§‡§æ ‡§π‡•Ç‡§Å ‡§ï‡§ø ‡§µ‡§π ‡§®‡§ø‡§∂‡•ç‡§ö‡§ø‡§§ ‡§∞‡•Ç‡§™ ‡§∏‡•á ‡§â‡§®‡§ï‡•Ä ‡§∏‡§≠‡•Ä ‡§ú‡§∞‡•Ç‡§∞‡§§‡•ã‡§Ç ‡§™‡§∞ ‡§ñ‡§∞‡§æ ‡§â‡§§‡§∞‡•á‡§ó‡§æ‡•§"
    },
    {
        "audio_name": "MAR_F (WIKI)",
        "audio_url": "https://github.com/AI4Bharat/IndicF5/raw/refs/heads/main/prompts/MAR_F_WIKI_00001.wav",
        "ref_text": "‡§¶‡§ø‡§ó‡§Ç‡§§‡§∞‡§æ‡§µ‡•ç‡§¶‡§æ‡§∞‡•á ‡§Ö‡§Ç‡§§‡§∞‡§æ‡§≥ ‡§ï‡§ï‡•ç‡§∑‡•á‡§§‡§≤‡§æ ‡§ï‡§ö‡§∞‡§æ ‡§ö‡§ø‡§®‡•ç‡§π‡§ø‡§§ ‡§ï‡§∞‡§£‡•ç‡§Ø‡§æ‡§∏‡§æ‡§†‡•Ä ‡§™‡•ç‡§∞‡§Ø‡§§‡•ç‡§® ‡§ï‡•á‡§≤‡•á ‡§ú‡§æ‡§§ ‡§Ü‡§π‡•á.",
        "synth_text": "‡§™‡•ç‡§∞‡§æ‡§∞‡§Ç‡§≠‡§ø‡§ï ‡§Ö‡§Ç‡§ï‡•Å‡§∞ ‡§õ‡•á‡§¶‡§ï. ‡§Æ‡•Ä ‡§∏‡•ã‡§≤‡§æ‡§™‡•Ç‡§∞ ‡§ú‡§ø‡§≤‡•ç‡§π‡•ç‡§Ø‡§æ‡§§‡•Ä‡§≤ ‡§Æ‡§æ‡§≥‡§∂‡§ø‡§∞‡§∏ ‡§§‡§æ‡§≤‡•Å‡§ï‡•ç‡§Ø‡§æ‡§§‡•Ä‡§≤ ‡§∂‡•á‡§§‡§ï‡§∞‡•Ä ‡§ó‡§£‡§™‡§§ ‡§™‡§æ‡§ü‡•Ä‡§≤ ‡§¨‡•ã‡§≤‡§§‡•ã‡§Ø. ‡§Æ‡§æ‡§ù‡•ç‡§Ø‡§æ ‡§ä‡§∏ ‡§™‡§ø‡§ï‡§æ‡§µ‡§∞ ‡§™‡•ç‡§∞‡§æ‡§∞‡§Ç‡§≠‡§ø‡§ï ‡§Ö‡§Ç‡§ï‡•Å‡§∞ ‡§õ‡•á‡§¶‡§ï ‡§ï‡•Ä‡§° ‡§Ü‡§¢‡§≥‡§§ ‡§Ü‡§π‡•á. ‡§ï‡•ç‡§≤‡•ã‡§∞‡§Å‡§ü‡•ç‡§∞‡§æ‡§®‡§ø‡§≤‡•Ä‡§™‡•ç‡§∞‡•ã‡§≤ (‡§ï‡•ã‡§∞‡§æ‡§ú‡•á‡§®) ‡§µ‡§æ‡§™‡§∞‡§£‡•á ‡§Ø‡•ã‡§ó‡•ç‡§Ø ‡§Ü‡§π‡•á ‡§ï‡§æ? ‡§§‡•ç‡§Ø‡§æ‡§ö‡•á ‡§™‡•ç‡§∞‡§Æ‡§æ‡§£ ‡§ï‡§ø‡§§‡•Ä ‡§Ö‡§∏‡§æ‡§µ‡•á?"
    },
    {
        "audio_name": "MAR_M (WIKI)",
        "audio_url": "https://github.com/AI4Bharat/IndicF5/raw/refs/heads/main/prompts/MAR_M_WIKI_00001.wav",
        "ref_text": "‡§Ø‡§æ ‡§™‡•ç‡§∞‡§•‡§æ‡§≤‡§æ ‡§è‡§ï‡•ã‡§£‡•Ä‡§∏‡§∂‡•á ‡§™‡§Ç‡§ö‡§æ‡§§‡§∞ ‡§à‡§∏‡§µ‡•Ä ‡§™‡§æ‡§∏‡•Ç‡§® ‡§≠‡§æ‡§∞‡§§‡•Ä‡§Ø ‡§¶‡§Ç‡§° ‡§∏‡§Ç‡§π‡§ø‡§§‡§æ‡§ö‡•Ä ‡§ß‡§æ‡§∞‡§æ ‡§ö‡§æ‡§∞‡§∂‡•á ‡§Ö‡§†‡•ç‡§†‡§æ‡§µ‡•Ä‡§∏ ‡§Ü‡§£‡§ø ‡§ö‡§æ‡§∞‡§∂‡•á ‡§è‡§ï‡•ã‡§£‡§§‡•Ä‡§∏‡§ö‡•ç‡§Ø‡§æ ‡§Ö‡§®‡•ç‡§§‡§∞‡•ç‡§ó‡§§ ‡§®‡§ø‡§∑‡•á‡§ß ‡§ï‡•á‡§≤‡§æ.",
        "synth_text": "‡§ú‡•Ä‡§µ‡§æ‡§£‡•Ç ‡§ï‡§∞‡§™‡§æ. ‡§Æ‡•Ä ‡§Ö‡§π‡§Æ‡§¶‡§®‡§ó‡§∞ ‡§ú‡§ø‡§≤‡•ç‡§π‡•ç‡§Ø‡§æ‡§§‡•Ä‡§≤ ‡§∞‡§æ‡§π‡•Å‡§∞‡•Ä ‡§ó‡§æ‡§µ‡§æ‡§§‡•Ç‡§® ‡§¨‡§æ‡§≥‡§æ‡§∏‡§æ‡§π‡•á‡§¨ ‡§ú‡§æ‡§ß‡§µ ‡§¨‡•ã‡§≤‡§§‡•ã‡§Ø. ‡§Æ‡§æ‡§ù‡•ç‡§Ø‡§æ ‡§°‡§æ‡§≥‡§ø‡§Ç‡§¨ ‡§¨‡§æ‡§ó‡•á‡§§ ‡§ú‡•Ä‡§µ‡§æ‡§£‡•Ç ‡§ï‡§∞‡§™‡§æ ‡§Æ‡•ã‡§†‡•ç‡§Ø‡§æ ‡§™‡•ç‡§∞‡§Æ‡§æ‡§£‡§æ‡§§ ‡§¶‡§ø‡§∏‡§§‡•ã‡§Ø. ‡§∏‡•ç‡§ü‡•ç‡§∞‡•á‡§™‡•ç‡§ü‡•ã‡§∏‡§æ‡§Ø‡§ï‡•ç‡§≤‡§ø‡§® ‡§Ü‡§£‡§ø ‡§ï‡•â‡§™‡§∞ ‡§ë‡§ï‡•ç‡§∏‡§ø‡§ï‡•ç‡§≤‡•ã‡§∞‡§æ‡§à‡§° ‡§´‡§µ‡§æ‡§∞‡§£‡•Ä‡§∏‡§æ‡§†‡•Ä ‡§Ø‡•ã‡§ó‡•ç‡§Ø ‡§™‡•ç‡§∞‡§Æ‡§æ‡§£ ‡§ï‡§æ‡§Ø ‡§Ö‡§∏‡§æ‡§µ‡•á?"
    },
]


# Preload all example audios
for example in EXAMPLES:
    sample_rate, audio_data = load_audio_from_url(example["audio_url"])
    example["sample_rate"] = sample_rate
    example["audio_data"] = audio_data


# Define Gradio interface with layout adjustments
with gr.Blocks() as iface:
    gr.Markdown(
        """
        # **IndicF5: High-Quality Text-to-Speech for Indian Languages**

        [![Hugging Face](https://img.shields.io/badge/HuggingFace-Model-orange)](https://huggingface.co/ai4bharat/IndicF5)

        We release **IndicF5**, a **near-human polyglot** **Text-to-Speech (TTS)** model trained on **1417 hours** of high-quality speech from **[Rasa](https://huggingface.co/datasets/ai4bharat/Rasa), [IndicTTS](https://www.iitm.ac.in/donlab/indictts/database), [LIMMITS](https://sites.google.com/view/limmits24/), and [IndicVoices-R](https://huggingface.co/datasets/ai4bharat/indicvoices_r)**.  

        IndicF5 supports **11 Indian languages**:  
        **Assamese, Bengali, Gujarati, Hindi, Kannada, Malayalam, Marathi, Odia, Punjabi, Tamil, Telugu.**  
        
        Generate speech using a reference prompt audio and its corresponding text.
        """
    )
    
    with gr.Row():
        with gr.Column():
            text_input = gr.Textbox(label="Text to Synthesize", placeholder="Enter the text to convert to speech...", lines=3)
            ref_audio_input = gr.Audio(type="numpy", label="Reference Prompt Audio")
            ref_text_input = gr.Textbox(label="Text in Reference Prompt Audio", placeholder="Enter the transcript of the reference audio...", lines=2)
            submit_btn = gr.Button("üé§ Generate Speech", variant="primary")
        
        with gr.Column():
            output_audio = gr.Audio(label="Generated Speech", type="numpy")
    
    # Add multiple examples
    examples = [
        [ex["synth_text"], (ex["sample_rate"], ex["audio_data"]), ex["ref_text"]] for ex in EXAMPLES
    ]
    
    gr.Examples(
        examples=examples,
        inputs=[text_input, ref_audio_input, ref_text_input],
        label="Choose an example:"
    )

    submit_btn.click(synthesize_speech, inputs=[text_input, ref_audio_input, ref_text_input], outputs=[output_audio])


iface.launch()
